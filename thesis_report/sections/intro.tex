\documentclass[../thesis.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

\section{Introduction}

	It is a well-known fact that deep learning suffers from susceptibility to adversarial examples \cite{adv_exp_original_paper}. These examples are optimized to fool the model into misclassifying them only using a small imperceptible amount of noise. They highlight essential shortcomings in the deep learning models and might help us understand them better. Any elements in the deep learning training pipeline can contribute to adversarial susceptibility. These elements are dataset, model architecture, loss function, and optimization method. In this work, I initially studied the effect of different self-supervised \textit{losses} and training methods on the adversarial robustness of deep learning models. Then I changed my focus to the \textit{dataset} and created two toy datasets to eliminate class ambiguity and ensure a robust solution to the problem exists. I trained a model on the toy datasets to observe the effect of using a controlled dataset on model robustness. Finally, I used the Causal3DIdent dataset \cite{3dident} to examine model robustness on a more realistic and complicated controlled dataset.   
	
	Current works regarding adversarial examples mostly focus on regularizers \cite{grad_align}, model geometry \cite{cure} or studying intrinsic robustness of  models and the features they extract \cite{features_not_bugs_madry}. However, few works focus on the effect of using different loss functions on the model's adversarial robustness. Self-supervised learning is a training method that encourages a model to extract useful features from samples without using labels. It has made significant improvements recently \cite{simclr_chen_simple_2020} by demonstrating performance on par with supervised learning methods. Some papers have demonstrated that self-supervised learning can improve different aspects of model robustness. \cite{adv_training_plus_ssl_training} shows that training a model using the sum of adversarial and self-supervised losses results in improved model robustness, \cite{contrast_to_divide} uses self-supervised training on datasets with noisy labels to train a supervised model with high accuracy, and \cite{label_noise_contrastive_learning} proves contrastive learning is robust to labeling noise. In addition, \cite{features_not_bugs_madry} suggests that the spurious correlation between labels and image features might be a culprit in adversarial susceptibility. Therefore, our initial hypothesis was that training a model using a self-supervised method could reduce the adversarial susceptibility by reducing the model's dependence on labels. Moreover, such a model might extract more robust image features. 
	
	To test this hypothesis in section \ref{sec:SSL_experiments_cifar10} I trained the same encoder network using supervised and different self-supervised training methods on the CIFAR10 classification task; this disproved the hypothesis. To investigate further I used toy and synthetic(Causal3DIdent) datasets in sections \ref{sec:toy_datasets} and \ref{sec:3dident} to control the causal factors that affect the dataset creation. The toy dataset consists of small pixel patterns moving on a small canvas. Using it, I could show that in an ideal setting, it is possible to train a 100\% robust model with respect to all attacks, using the standard supervised training methods (SGD and crossentropy loss) by just adding the appropriate samples to the dataset. Causal3DIdent is a synthetic dataset created in blender which uses seven objects and different scene properties such as object color, rotation, and lighting angle to render varied images of objects. On Causal3DIdent, I show that even though a model trained using a self-supervised method can retrieve the causal factors of the samples accurately; it can still have an adversarial accuracy of 0\%. i.e., Although the model can predict the variables that created an image in the first place, it does not necessarily entail that it is robust.
\end{document}