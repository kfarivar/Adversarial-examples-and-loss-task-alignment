\documentclass[../thesis.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

\section{Knowing the causes is not enough ! (Causal3DIdent experiments)}
\label{sec:3dident}

The previous toy datasets only used simple basic pixel patterns as the causal factor to create a small dataset. Although they provided the possibility of running computational methods that are infeasible for real image datasets, their features didn't represent the complexity of real-world image classification tasks. To investigate further I used the Causal3DIdent dataset (a.k.a 3dident) from \cite{3dident}. Compared to the toy datasets Causal3DIdent is much closer to real world images which can lead to better insights regarding the factors that affect model robustness on real datasets. Causal3DIdent's causal graph is shown in \ref{fig:3dident_explain}, each independent causal factor on the top row controls different features of the scene (e.g spotlight position or object-class) which can either be considered as the feature of interest for the model to regress/predict or contribute to the calculation of other dependent factors (e.g. object-position). This causal structure would give us the appropriate controls needed to experiment with different hypotheses using the dataset. There is real and rendered versions of the dataset but only the rendered version is available.   
 
\subsection{Dataset creation}
The dataset is created using 4 main causal factors: spotlight position ($pos_{spl}$), hue ($hue_{spl}$), object class and background hue ($hue_{bg}$). These variables are all sampled independently form $U[-1,1]$. All object variables are dependent on these initial variables by changing the mean of a normal distribution (for more details see appendix B of \cite{3dident}). In total there are 7 object classes: Teapot, Hare, Dragon, Cow, Armadillo, Horse and Head. The images are 224 Ã— 224 pixels and are created using blender. Each object is rendered using the following attributes:

\begin{multicols}{3}
\begin{enumerate}
	\item $x \in [-3, 3]$
	\item $y \in [-3, 3]$
	\item $z \in[-3, 3]$
	\item $\alpha \in [0, 2\pi]$
	\item $\beta \in [0, 2\pi]$
	\item $\gamma \in [0, 2\pi]$
	\item $\theta \in [0, \pi]$
	\item hue object $\in [0, 2\pi]$
	\item hue spotlight $\in[0, 2\pi]$
\end{enumerate}
\end{multicols}
$(x,y,z)$ are object the position, $(\alpha, \beta, \gamma)$ are the object rotation and $\theta$ is spotlight rotation. There is also hue background in $[0, 2\pi]$ which is changed per scene.



\begin{figure}[h!]
	\centering	
	\includegraphics[width=1\linewidth]{data_gen.png}
	\caption{(Left) Causal graph for the Causal3DIdent dataset. (Right) Two samples from each object class.}
	\label{fig:3dident_explain}
\end{figure}




\subsection{General idea}
\label{sec:3dident_general_idea}

Up until this point we only considered classification tasks where there is no objective meaningful metric defined for the distance between classes. But in 3dident we have the opportunity to regress a continuous random variable that is a causal factor for creating images. We can then divide this variable into separate regions (according to the specific hypothesis we want to test) to create discrete classes. For instance we can consider the spotlight rotation and discretize it into 3 non-overlapping classes as in \ref{fig:spotlight_rotation_classes} (the spotlight is rotated on a semicircle above the object see \ref{sec:spotlight_exp} for details). I then define the parameter \textbf{$\delta^*$} as the maximum perturbation that can be tolerated by the \textit{classification task} before values from one class end up in a region belonging to another class as shown in \ref{fig:spotlight_rotation_classes}. Note that $\delta^*$ is purely a property of the classification task and has nothing to do with the network architecture, loss or the optimization method used to train it. This setup makes it possible to separate $\delta^*$ from the model robustness. In case of an inherently discrete object classification task (e.g. cat vs dog) one can think of estimating the output probabilities as a regression task but there is no well defined distance or similarity metric, so it is hard to calculate the $\delta^*$ for such a dataset.



\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{regression_to_classification.pdf}
	\caption{The spotlight rotation is divided into 3 non-overlapping classes. Each class has the same range and the gaps between classes are equal in length. Spotlight rotation is sampled uniformly from the whole range $[0,\pi]$ so the classes are balanced. Since the classes don't overlap there exists a model that is robust. The most robust classifier would separate the classes using the middle point of gaps (red dashed lines). Therefore $\delta^*$ is the maximum perturbation that can be tolerated by this classification task.}
	\label{fig:spotlight_rotation_classes}
\end{figure}

Discretizing a regression task into a classification task gives us much more control over how the classes are defined, whether they are ambiguous (the classes overlap in the regression space) or well separated (like in \ref{fig:spotlight_rotation_classes}) and how many classes we have. This also makes characterizing adversarial examples simpler. Furthermore the model can be divided into two parts: The network that regresses the continuous variable and a classification head added on top. Note that the classification head is only a function of the classification task we define and doesn't need to be trained. To provably determine the amount of perturbation this classifier is robust to we only need $\delta^*$ and the lipschitz constant of the network $\lambda$. 

\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{regressor_classifier_network.pdf}
	\caption{The classifier including the regressor and the classification head. Note that the classification head is purely defined by our definition of classes and is not learned.}
	\label{fig:regressor_classifier}
\end{figure}

Assuming the network is $\lambda$-lipschitz and the input is $x$ and the bounded perturbation vector is $r$ s.t. $\norm{r} \leq \epsilon$ making the adversarial image $x+r$ we have: 

\begin{align}
	\forall x \in \mathbb{R}^n \quad &f(x)\in \mathbb{R} \\
	\forall x,r \in \mathbb{R}^n \quad & |f(x) - f(x+r)| \leq 
	\lambda \norm{r}  \leq   \lambda  \epsilon \\
	& \lambda  \epsilon \leq \delta^*
\end{align}

The most perturbation that the classification head input can tolerate, and for the sample not to pass the classification boundaries (red lines in \ref{fig:spotlight_rotation_classes}) is $\delta^*$ thus the third line in the equations above. Given $\delta^*$ and $\lambda$ of a specific network we can provably say that our classifier is 100\% robust against any attack where the perturbation is bounded as $\epsilon < \frac{\delta^*}{\lambda}$. Similarly given $\delta^*$ and $\epsilon$ we can calculate an upper bound for the lipschitz constant of the network required for the whole model to be 100\% robust in this classification task
\footnote{ We can also use $\lambda  \epsilon \leq \delta^*$ to estimate the lipschitz constant of the network (upper bound it), fixing the $\delta^*$ we keep decreasing $\epsilon$ until the robust classification accuracy is 100\%. The more points sampled from the data distribution the better the estimate. The strength of the attack also affects the quality of the estimated value.}.

% the below statement is incorrect.
%\sout{Another advantage of this method of creating a classification task is that it eliminates the problem of out of distribution adversarial samples mentioned in \cite{on_manifold_off_manifold} since the causal factor of interest (e.g. spotlight rotation) used to create the samples ,which are used in training the regression model, are uniformly sampled from the region of interest (e.g. $[0,\pi]$) so there are no out of distribution samples.} 

This method could also be generalized to a regression network with a multidimensional output. The practical issue with using this setting is finding a method to train the network such that its lipschitz constant is bounded by a specific value $\lambda$. Usually even estimating the lipschitz constant of a network can be computationally expensive. However, if we manage to train such a network for the regression task then we would have a 100\% robust model for the classification task. Possibly a more useful aspect of the setting in \ref{fig:regressor_classifier} is to test if a model + training method can produce a robust model. For instance in my experiments I asked:

\begin{center}
\textbf{Does a SSL model learn a more robust representation than a supervised model on this controlled dataset?}   
\end{center}

\subsection{Experiments}
\label{sec:spotlight_exp}

\cite{3dident} argues that discriminative learning methods such as simclr can be used to disentangle the underlying causal factors of a dataset. They create the 3dident dataset as a toy dataset to validate their theory. I initially tested if I can train the simclr model with similar performance as the paper \cite{3dident}. 

The simclr model is trained to extract image features using self supervision. I then train a linear readout layer separately (similar to \ref{fig:regressor_classifier}) and regress the causal factors. For continuous causal factors the performance of simclr is measured by $R^2$ (coefficient of determination). To calculate $R^2$ for a regression task involving $n$ variables with actual values $\{ y_1, y_2, ..., y_n \}$ and regressed values $\{ f_1, f_2, ..., f_n \}$ we have: 

\begin{align}
	\bar{y}&=\frac{1}{n}\sum_{i=1}^n y_i \\
	SS_\text{res}&=\sum_i (y_i - f_i)^2=\sum_i e_i^2 \\
	SS_\text{tot}&=\sum_i (y_i - \bar{y})^2 \\
	R^2 &= 1 - {SS_{\rm res}\over SS_{\rm tot}} 
\end{align} 

Where residuals are defined as $e_i = y_i - f_i$, $SS_{res}$ is the residual sum of squares and $SS_{tot}$ is the total sum of squares. Note that $SS_{tot}$ is only a function of the data and constant w.r.t the predictions. This means that the less residual we have for each prediction the lower $SS_{res}$ while $SS_{tot}$ is constant so $R^2$ is closer to 1. Note that we can have $SS_{tot} < SS_{res}$ which means our predictions $f_i$s are even worse than predicting the average of $y_i$s for all the points and this results in a negative value for $R^2$. I used accuracy to measure object classification performance. All the measurements were done using standard images and the results can be see in \ref{tbl:R2_results}. For simclr I chose the small random crop and color distortion data augmentations since they provided the best  $R^2$ score for spotlight position (rotation). As we can see the results are very close to that of the paper \cite{3dident} table 1. Specifically the spotlight angle ($\theta$) can be regressed with low error.  

\begin{table}
	\centering
	\npdecimalsign{.}
	\nprounddigits{3}
	\begin{tabular}{ *{9} {|n{2}{3}} |}
		\hline
		\text{Readout layer}&\text{Object Accuracy} & \text{x} & \text{y} & \text{z} & \text{$\alpha$} & \text{$\beta$} & \text{$\gamma$} & \text{$\theta$} \\
		\hline
		
		\text{linear readout}& 1.0& 0.8639570873788722 & 0.7912421253663532& 0.7916736909965967& 0.6205633304872211 & 0.5270109695267875& 0.5334545237997002& 0.9620300509820501\\
		\hline 
		
		\text{kernel ridge} & 1.0 & 0.7295743842761302& 0.6579604724328898& 0.670759680802943& 0.5815572339950723& 0.49690154398165587& 0.5688862883373637& 0.9216010758158096\\
		\hline
	\end{tabular}
	\npnoround
	\caption{The $R^2$ scores for different image features for Simclr. First row results for a linear readout, second row corresponds to using kernel ridge regression. Note since I used color distortion in training simclr the scores for the 3 color related attributes (object, spotlight, background) were negative or close to 0 and excluded from the table. We can see that linear readout gives the best results.}
	
	\label{tbl:R2_results}
\end{table}

I also included a supervised model to compare to simclr. Similar to the CIFAR10 experiments I used lightening bolts Resnet18 as the backbone for both supervised and simclr training. Unlike CIFAR10 since the images in 3dident are much bigger, the first convolutional layer reduces the output resolution by half (112x112) this doesn't affect the results in \ref{tbl:R2_results} and makes training much faster. One possible choice for the continuous variable of interest is spotlight rotation $\theta \in [-\pi/2, \pi/2]$. The spotlight position as a function of $\theta$ is: 

\begin{align}
(4*sin(\theta), 4*cos(\theta), 6+max\_object\_size)
\end{align}
 
where "max\_object\_size" is the maximum dimension of the object with the biggest dimensions and is a constant. This means the spotlight moves in a half circle above the object shining at it in an angle. $\theta$ is originally uniformly sampled from $[-1,1]$ and linearly mapped to $[-\pi/2, \pi/2]$. To create a classification task I discretized $\theta$ into 3 non-overlapping classes: $[\pi/4, \pi/2]$, $[-\pi/8, \pi/8]$, $[-\pi/2, -\pi/4]$ similar to \ref{fig:spotlight_rotation_classes}. The interval lengths are equal so the classes are balanced.    
\todo[inline]{Add a figure for the semi circle, use blender (low priority)}   

I repeated the same training process as CIFAR10 to train SimCLR and Supervised models (including retraining the last layer for supervised which had the same results). Note that here, unlike the previous section, for both models, the readout layer is trained as a classifier and not a regressor. Since my readout layer for regression in \ref{tbl:R2_results} was a simple linear layer, from the perspective of model capacity having a linear classification readout is the same as first linearly regressing spotlight and then classifying it. The supervised model was trained only on 4\% of the data since that was enough to reach an accuracy above 99\% however, Simclr was trained on the whole trainset. The attacks had $\epsilon = 8/255$ with $l_{\infty}$ norm. The results are:


 \begin{enumerate}
 	\item Both models had standard accuracies in $[0.99,1]$ interval.
 	
 	\item All had robust accuracy 0 \% for APGD with 100 iterations and all have accuracy under 1\% for APGD 10 iterations.
 	
 	\item The simclr model had an accuracy of 33\% against FGSM, while the supervised model's accuracy was 1\%. As expected this shows FGSM is much less accurate than APGD in solving the optimization problem .
 	
 	%\item The feature norm difference for APGD:\quad Simclr:14 \quad supervised:95. For FGSM: \quad Simclr:5 \quad supervised:14.  
 \end{enumerate}

In conclusion, just because contrastive methods such as simclr can retrieve the causal factors accurately it doesn't make these models any more robust than supervised models. This might be due to a well known phenomena called \textbf{underspecification}. An MLpipeline is underspecified if there are many distinct ways (e.g., different weight configurations) forthe model to achieve equivalent held-out performance on iid data, even if the model specification andtraining data are held constant \cite{underspecification}. We could already observe this in the L-T dataset when training the model on only the 8 standard images. There, both of the robust and non-robust models had 100\% standard accuracy, but completely opposite robust accuracies (0\% and 100\%). This means the problem as represented by the standard dataset was underspecified. So in order to train a robust model we need to add more conditions or constraints to the training pipeline. It might be necessary to extend the dataset with lots of data (similar to the L-T dataset extension which can be seen as a from of data augmentation) or adding more assumptions via a special model architecture, loss modification or regularization.  
 
One major shortcoming of designing the experiment around spotlight rotation is that it's hard for humans to classify it by looking at the images while the computer does much better. This makes it much harder to intuitively reason about the shortcomings of the model, dataset or the proper amount of perturbation. So I used Blender to create a similar dataset that is based on regressing and classifying the distance between objects making it more human understandable. Future experiments can take advantage of such a dataset.  

\todo[inline]{1. also calculate adversarially training the readout for both models to give them best chance. 2. also try the attack training and eval with epsilons: 1/256 and 2/256 these should be below or same as minimum increase in pixel values ! How to determine if the amount of adversarial perturbation is not too much for spotlight classification ? since 8/225 is less than the accuracy of jpg format! (low priority)} 

\end{document}