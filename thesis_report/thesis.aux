\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{none/global//global/global}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\babel@aux{english}{}
\abx@aux@cite{adv_exp_original_paper}
\abx@aux@segm{0}{0}{adv_exp_original_paper}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\abx@aux@cite{grad_align}
\abx@aux@segm{0}{0}{grad_align}
\abx@aux@cite{cure}
\abx@aux@segm{0}{0}{cure}
\abx@aux@cite{features_not_bugs_madry}
\abx@aux@segm{0}{0}{features_not_bugs_madry}
\abx@aux@cite{simclr_chen_simple_2020}
\abx@aux@segm{0}{0}{simclr_chen_simple_2020}
\abx@aux@cite{adv_training_plus_ssl_training}
\abx@aux@segm{0}{0}{adv_training_plus_ssl_training}
\abx@aux@cite{contrast_to_divide}
\abx@aux@segm{0}{0}{contrast_to_divide}
\abx@aux@cite{label_noise_contrastive_learning}
\abx@aux@segm{0}{0}{label_noise_contrastive_learning}
\abx@aux@cite{features_not_bugs_madry}
\abx@aux@segm{0}{0}{features_not_bugs_madry}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\abx@aux@cite{goodfellow_fgsm}
\abx@aux@segm{0}{0}{goodfellow_fgsm}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Adversarial examples}{4}{subsection.2.1}\protected@file@percent }
\newlabel{def_adv_exp}{{2.1}{4}{Adversarial examples}{subsection.2.1}{}}
\newlabel{def_adv_exp@cref}{{[subsection][1][2]2.1}{[1][4][]4}}
\newlabel{eqn:adversarial_example_equation}{{1}{4}{Adversarial examples}{equation.2.1}{}}
\newlabel{eqn:adversarial_example_equation@cref}{{[equation][1][]1}{[1][4][]4}}
\abx@aux@cite{autoattack}
\abx@aux@segm{0}{0}{autoattack}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\abx@aux@cite{on_manifold_off_manifold}
\abx@aux@segm{0}{0}{on_manifold_off_manifold}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Adversarial training}{5}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Adversarial examples, and their (normalized) difference to the original image, in the context of the underlying manifold, eg. class manifolds ``5'' and ``6'' on EMNIST, allow to study their relation to generalization. Regular adversarial examples are not constrained to the manifold, cf. (a), and often result in (seemingly) random noise patterns; in fact, we show that they leave the manifold. However, adversarial examples on the manifold can be found as well, cf. (b), resulting in meaningful manipulations of the image content; however, care needs to be taken that the actual, true label w.r.t the manifold does not change, cf. (c).\relax }}{6}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:introduction}{{1}{6}{Adversarial examples, and their (normalized) difference to the original image, in the context of the underlying manifold, eg. class manifolds ``5'' and ``6'' on EMNIST, allow to study their relation to generalization. Regular adversarial examples are not constrained to the manifold, cf. (a), and often result in (seemingly) random noise patterns; in fact, we show that they leave the manifold. However, adversarial examples on the manifold can be found as well, cf. (b), resulting in meaningful manipulations of the image content; however, care needs to be taken that the actual, true label w.r.t the manifold does not change, cf. (c).\relax }{figure.caption.2}{}}
\newlabel{fig:introduction@cref}{{[figure][1][]1}{[1][5][]6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}On-manifold vs Off-manifold Adversarial Robustness}{6}{subsection.2.3}\protected@file@percent }
\abx@aux@cite{rotation_gidaris_unsupervised_2018}
\abx@aux@segm{0}{0}{rotation_gidaris_unsupervised_2018}
\abx@aux@cite{arora_theoretical_2019}
\abx@aux@segm{0}{0}{arora_theoretical_2019}
\abx@aux@cite{simclr_chen_simple_2020}
\abx@aux@segm{0}{0}{simclr_chen_simple_2020}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:experimentshypo1b}{{2}{7}{\relax }{figure.caption.3}{}}
\newlabel{fig:experimentshypo1b@cref}{{[figure][2][]2}{[1][6][]7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Self-supervised learning (SSL)}{7}{subsection.2.4}\protected@file@percent }
\newlabel{sec:ssl_methods}{{2.4}{7}{Self-supervised learning (SSL)}{subsection.2.4}{}}
\newlabel{sec:ssl_methods@cref}{{[subsection][4][2]2.4}{[1][7][]7}}
\abx@aux@cite{barlow_twins}
\abx@aux@segm{0}{0}{barlow_twins}
\newlabel{eq:loss}{{11}{8}{Self-supervised learning (SSL)}{equation.2.11}{}}
\newlabel{eq:loss@cref}{{[equation][11][]11}{[1][8][]8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces SimCLR, a simple framework for contrastive learning of visual representations. Two separate data augmentation operators are sampled from the same family of augmentations ($t\sim \mathcal  {T}$ and $t'\sim \mathcal  {T}$) and applied to each data example to obtain two correlated views. A base encoder network $f(\cdot )$ and a projection head $g(\cdot )$ are trained to maximize agreement using a contrastive loss. After training is completed, we throw away the projection head $g(\cdot )$ and use encoder $f(\cdot )$ and representation $\bm  {h}$ for downstream tasks.\relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:simclr_framework}{{3}{8}{SimCLR, a simple framework for contrastive learning of visual representations. Two separate data augmentation operators are sampled from the same family of augmentations ($t\sim \mathcal {T}$ and $t'\sim \mathcal {T}$) and applied to each data example to obtain two correlated views. A base encoder network $f(\cdot )$ and a projection head $g(\cdot )$ are trained to maximize agreement using a contrastive loss. After training is completed, we throw away the projection head $g(\cdot )$ and use encoder $f(\cdot )$ and representation $\bm h$ for downstream tasks.\relax }{figure.caption.4}{}}
\newlabel{fig:simclr_framework@cref}{{[figure][3][]3}{[1][8][]8}}
\newlabel{eq:lossBarlow}{{12}{9}{Self-supervised learning (SSL)}{equation.2.12}{}}
\newlabel{eq:lossBarlow@cref}{{[equation][12][]12}{[1][9][]9}}
\newlabel{eq:crosscorr}{{13}{9}{Self-supervised learning (SSL)}{equation.2.13}{}}
\newlabel{eq:crosscorr@cref}{{[equation][13][]13}{[1][9][]9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Barlow Twins' objective function measures the cross-correlation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. Barlow Twins is competitive with state-of-the-art methods for self-supervised learning while being conceptually simpler, naturally avoiding trivial constant (i.e. collapsed) embeddings, and being robust to the training batch size.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:barlow_twins_method}{{4}{9}{Barlow Twins' objective function measures the cross-correlation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. Barlow Twins is competitive with state-of-the-art methods for self-supervised learning while being conceptually simpler, naturally avoiding trivial constant (i.e. collapsed) embeddings, and being robust to the training batch size.\relax }{figure.caption.5}{}}
\newlabel{fig:barlow_twins_method@cref}{{[figure][4][]4}{[1][9][]9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {SimSiam architecture}. Two augmented views of one image are processed by the same encoder network $f$ (a backbone plus a projection MLP). Then a prediction MLP $h$ is applied on one side, and a stop-gradient operation is applied on the other side. The model maximizes the similarity between both sides. It uses neither negative pairs nor a momentum encoder.  \vspace  {-.5em} \relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:simsiam_pipeline}{{5}{10}{\textbf {SimSiam architecture}. Two augmented views of one image are processed by the same encoder network $f$ (a backbone plus a projection MLP). Then a prediction MLP $h$ is applied on one side, and a stop-gradient operation is applied on the other side. The model maximizes the similarity between both sides. It uses neither negative pairs nor a momentum encoder.  \vspace {-.5em} \relax }{figure.caption.6}{}}
\newlabel{fig:simsiam_pipeline@cref}{{[figure][5][]5}{[1][10][]10}}
\newlabel{eq:dist_cosine}{{14}{10}{Self-supervised learning (SSL)}{equation.2.14}{}}
\newlabel{eq:dist_cosine@cref}{{[equation][14][]14}{[1][10][]10}}
\newlabel{eq:loss_sym}{{15}{10}{Self-supervised learning (SSL)}{equation.2.15}{}}
\newlabel{eq:loss_sym@cref}{{[equation][15][]15}{[1][10][]10}}
\newlabel{eq:loss_asym_stopgrad}{{16}{10}{Self-supervised learning (SSL)}{equation.2.16}{}}
\newlabel{eq:loss_asym_stopgrad@cref}{{[equation][16][]16}{[1][10][]10}}
\newlabel{eq:loss_sym_stopgrad}{{17}{10}{Self-supervised learning (SSL)}{equation.2.17}{}}
\newlabel{eq:loss_sym_stopgrad@cref}{{[equation][17][]17}{[1][10][]10}}
\abx@aux@cite{arora_theoretical_2019}
\abx@aux@segm{0}{0}{arora_theoretical_2019}
\abx@aux@cite{contrastive_learning_spectral_proof}
\abx@aux@segm{0}{0}{contrastive_learning_spectral_proof}
\abx@aux@cite{supervised_texture_bias}
\abx@aux@segm{0}{0}{supervised_texture_bias}
\abx@aux@cite{contrastive_loss_shortcut_solutions}
\abx@aux@segm{0}{0}{contrastive_loss_shortcut_solutions}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Shortcut Solutions and Feature Suppression in SSL}{11}{subsection.2.5}\protected@file@percent }
\newlabel{sec:model_shortcuts}{{2.5}{11}{Shortcut Solutions and Feature Suppression in SSL}{subsection.2.5}{}}
\newlabel{sec:model_shortcuts@cref}{{[subsection][5][2]2.5}{[1][11][]11}}
\abx@aux@cite{adv_training_plus_ssl_training}
\abx@aux@segm{0}{0}{adv_training_plus_ssl_training}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\abx@aux@cite{rotation_gidaris_unsupervised_2018}
\abx@aux@segm{0}{0}{rotation_gidaris_unsupervised_2018}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A sample from DigitOnImageNet\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:digitonimagenetoverlayexample1}{{6}{12}{A sample from DigitOnImageNet\relax }{figure.caption.7}{}}
\newlabel{fig:digitonimagenetoverlayexample1@cref}{{[figure][6][]6}{[1][12][]12}}
\newlabel{fig:linear_overlay_sup}{{7a}{12}{Supervised learning\relax }{figure.caption.8}{}}
\newlabel{fig:linear_overlay_sup@cref}{{[subfigure][1][7]7a}{[1][12][]12}}
\newlabel{sub@fig:linear_overlay_sup}{{a}{12}{Supervised learning\relax }{figure.caption.8}{}}
\newlabel{sub@fig:linear_overlay_sup@cref}{{[subfigure][1][7]7a}{[1][12][]12}}
\newlabel{fig:linear_overlay_unsup}{{7b}{12}{Unsupervised contrastive learning\relax }{figure.caption.8}{}}
\newlabel{fig:linear_overlay_unsup@cref}{{[subfigure][2][7]7b}{[1][12][]12}}
\newlabel{sub@fig:linear_overlay_unsup}{{b}{12}{Unsupervised contrastive learning\relax }{figure.caption.8}{}}
\newlabel{sub@fig:linear_overlay_unsup@cref}{{[subfigure][2][7]7b}{[1][12][]12}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  (a) Supervised learning accuracy on ImageNet classification. (b) Linear evaluation of learned features for both MNIST classification and ImageNet classification on the DigitOnImageNet dataset. Batch size of 1024 and 2-layer projection head is used. Different batch sizes and projection head layers have negligible influence on the trade-off between ImageNet vs MNIST accuracy.\relax }}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:simclr_shortcut_solution}{{7}{12}{(a) Supervised learning accuracy on ImageNet classification. (b) Linear evaluation of learned features for both MNIST classification and ImageNet classification on the DigitOnImageNet dataset. Batch size of 1024 and 2-layer projection head is used. Different batch sizes and projection head layers have negligible influence on the trade-off between ImageNet vs MNIST accuracy.\relax }{figure.caption.8}{}}
\newlabel{fig:simclr_shortcut_solution@cref}{{[figure][7][]7}{[1][12][]12}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Robustness of contrastive learning}{12}{subsection.2.6}\protected@file@percent }
\newlabel{sec:Robustness_contrastive_learning}{{2.6}{12}{Robustness of contrastive learning}{subsection.2.6}{}}
\newlabel{sec:Robustness_contrastive_learning@cref}{{[subsection][6][2]2.6}{[1][12][]12}}
\abx@aux@cite{contrast_to_divide}
\abx@aux@segm{0}{0}{contrast_to_divide}
\abx@aux@cite{ELR}
\abx@aux@segm{0}{0}{ELR}
\abx@aux@cite{label_noise_contrastive_learning}
\abx@aux@segm{0}{0}{label_noise_contrastive_learning}
\abx@aux@cite{ifm_ssl_avoid_shortcuts}
\abx@aux@segm{0}{0}{ifm_ssl_avoid_shortcuts}
\abx@aux@cite{ACL_Adversarial_Contrastive_Learning_2020}
\abx@aux@segm{0}{0}{ACL_Adversarial_Contrastive_Learning_2020}
\abx@aux@cite{ifm_ssl_avoid_shortcuts}
\abx@aux@segm{0}{0}{ifm_ssl_avoid_shortcuts}
\newlabel{eqn:adv_loss_SSL_loss}{{18}{13}{Robustness of contrastive learning}{equation.2.18}{}}
\newlabel{eqn:adv_loss_SSL_loss@cref}{{[equation][18][]18}{[1][13][]13}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Robust Self-Supervised Training}{13}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Can contrastive learning avoid shortcut solutions?}{13}{subsubsection.2.7.1}\protected@file@percent }
\newlabel{sec:ifm_ssl_shortcuts}{{2.7.1}{13}{Can contrastive learning avoid shortcut solutions?}{subsubsection.2.7.1}{}}
\newlabel{sec:ifm_ssl_shortcuts@cref}{{[subsubsection][1][2,7]2.7.1}{[1][13][]13}}
\abx@aux@cite{shortcut_in_contrastive_losses}
\abx@aux@segm{0}{0}{shortcut_in_contrastive_losses}
\abx@aux@cite{ACL_Adversarial_Contrastive_Learning_2020}
\abx@aux@segm{0}{0}{ACL_Adversarial_Contrastive_Learning_2020}
\newlabel{principle: motivating adv framework}{{2.7.1}{14}{Can contrastive learning avoid shortcut solutions?}{subsubsection.2.7.1}{}}
\newlabel{principle: motivating adv framework@cref}{{[subsubsection][1][2,7]2.7.1}{[1][14][]14}}
\newlabel{eqn: attack both loss}{{19}{14}{Can contrastive learning avoid shortcut solutions?}{equation.2.19}{}}
\newlabel{eqn: attack both loss@cref}{{[equation][19][]19}{[1][14][]14}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An example of the feature space. The standard image x is mapped by the model in the feature space to f(x). The ball considered by IFM ($f(x) +\delta $) is shown in red (assuming $l_2$ norm). The neighborhood covered by the adversarial perturbation is shown in green ($f(x+\delta )$). The black dashed line is the correct boundary and the blue dashed line is the boundary learned by the model. $f(x')$ is the worst case adversarial example found using an attack. The orange curve is the radius of the ball if IFM wanted to include the adversarial sample $f(x')$. This would obviously force the model to incorrectly assign many samples to the same class as f(x).\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{fig:advinput_vs_ifm}{{8}{15}{An example of the feature space. The standard image x is mapped by the model in the feature space to f(x). The ball considered by IFM ($f(x) +\delta $) is shown in red (assuming $l_2$ norm). The neighborhood covered by the adversarial perturbation is shown in green ($f(x+\delta )$). The black dashed line is the correct boundary and the blue dashed line is the boundary learned by the model. $f(x')$ is the worst case adversarial example found using an attack. The orange curve is the radius of the ball if IFM wanted to include the adversarial sample $f(x')$. This would obviously force the model to incorrectly assign many samples to the same class as f(x).\relax }{figure.caption.9}{}}
\newlabel{fig:advinput_vs_ifm@cref}{{[figure][8][]8}{[1][14][]15}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Robust Pre-Training by Adversarial Contrastive Learning}{15}{subsubsection.2.7.2}\protected@file@percent }
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces (a) The original SimCLR framework, a.k.a., standard to standard (no adversarial attack involved); (b) - (d) three proposed variants of the adversarial contrastive learning framework: A2A, A2S, and DS (their best solution). Note that, whenever more than one encoder branches co-exist in one framework, they by default share all weights, except that adversarial and standard encoders will use independent BN parameters.\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ACL_methods}{{9}{16}{(a) The original SimCLR framework, a.k.a., standard to standard (no adversarial attack involved); (b) - (d) three proposed variants of the adversarial contrastive learning framework: A2A, A2S, and DS (their best solution). Note that, whenever more than one encoder branches co-exist in one framework, they by default share all weights, except that adversarial and standard encoders will use independent BN parameters.\relax }{figure.caption.10}{}}
\newlabel{fig:ACL_methods@cref}{{[figure][9][]9}{[1][16][]16}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}SSL and Latent Variable Disentanglement}{16}{subsection.2.8}\protected@file@percent }
\newlabel{sec:disentanglement}{{2.8}{16}{SSL and Latent Variable Disentanglement}{subsection.2.8}{}}
\newlabel{sec:disentanglement@cref}{{[subsection][8][2]2.8}{[1][16][]16}}
\abx@aux@cite{grad_align}
\abx@aux@segm{0}{0}{grad_align}
\abx@aux@cite{cure}
\abx@aux@segm{0}{0}{cure}
\abx@aux@cite{features_not_bugs_madry}
\abx@aux@segm{0}{0}{features_not_bugs_madry}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The latent variable $z$ is partitioned into content~$c$ and style~$s$, and they allow for statistical and causal dependence of style on content(purple arrow). It is assumed that only style changes (green arrow) between the original view $x$ and the augmented view $\tilde  {x}$, i.e., they are obtained by applying the same deterministic function (orange arrows) $f$ to $z=(c,s)$ and $\tilde  {z}=(c,\tilde  {s})$.\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:3didentstylechangegraph}{{10}{17}{The latent variable $z$ is partitioned into content~$c$ and style~$s$, and they allow for statistical and causal dependence of style on content(purple arrow). It is assumed that only style changes (green arrow) between the original view $x$ and the augmented view $\tilde {x}$, i.e., they are obtained by applying the same deterministic function (orange arrows) $f$ to $z=(c,s)$ and $\tilde {z}=(c,\tilde {s})$.\relax }{figure.caption.11}{}}
\newlabel{fig:3didentstylechangegraph@cref}{{[figure][10][]10}{[1][16][]17}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Hypothesis}{17}{section.3}\protected@file@percent }
\newlabel{sec:hypothesis}{{3}{17}{Hypothesis}{section.3}{}}
\newlabel{sec:hypothesis@cref}{{[section][3][]3}{[1][17][]17}}
\abx@aux@cite{features_not_bugs_madry}
\abx@aux@segm{0}{0}{features_not_bugs_madry}
\abx@aux@cite{robustness_at_odds_madry}
\abx@aux@segm{0}{0}{robustness_at_odds_madry}
\abx@aux@cite{excessive_invariance}
\abx@aux@segm{0}{0}{excessive_invariance}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Self-supervised learning, not robust on CIFAR10 }{18}{section.4}\protected@file@percent }
\newlabel{sec:SSL_experiments_cifar10}{{4}{18}{Self-supervised learning, not robust on CIFAR10}{section.4}{}}
\newlabel{sec:SSL_experiments_cifar10@cref}{{[section][4][]4}{[1][18][]18}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Many causal factors can contribute to creating a dataset; these are shown with brown boxes on the top left. To create classification tasks, humans usually label samples, so they introduce their own biases to the dataset via the labels. The blue boundary shows natural data separation; the red boundary is distorted due to labeling noise which could contribute to adversarial susceptibility. Finally, we sample the dataset to train the model according to the labels but evaluate on unseen samples from the real distribution. Could this contribute to model's adversarial susceptibility ?\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:data_seperation_human_labeling_and_training}{{11}{19}{Many causal factors can contribute to creating a dataset; these are shown with brown boxes on the top left. To create classification tasks, humans usually label samples, so they introduce their own biases to the dataset via the labels. The blue boundary shows natural data separation; the red boundary is distorted due to labeling noise which could contribute to adversarial susceptibility. Finally, we sample the dataset to train the model according to the labels but evaluate on unseen samples from the real distribution. Could this contribute to model's adversarial susceptibility ?\relax }{figure.caption.12}{}}
\newlabel{fig:data_seperation_human_labeling_and_training@cref}{{[figure][11][]11}{[1][18][]19}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces SSL model parts.\relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:encoderreadout}{{12}{19}{SSL model parts.\relax }{figure.caption.13}{}}
\newlabel{fig:encoderreadout@cref}{{[figure][12][]12}{[1][18][]19}}
\abx@aux@cite{bolts}
\abx@aux@segm{0}{0}{bolts}
\abx@aux@cite{bolts}
\abx@aux@segm{0}{0}{bolts}
\abx@aux@cite{huy_phan_2021_4431043}
\abx@aux@segm{0}{0}{huy_phan_2021_4431043}
\abx@aux@cite{simclr_chen_simple_2020}
\abx@aux@segm{0}{0}{simclr_chen_simple_2020}
\abx@aux@cite{simsiam}
\abx@aux@segm{0}{0}{simsiam}
\abx@aux@cite{barlow_twins}
\abx@aux@segm{0}{0}{barlow_twins}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Models and training}{20}{subsection.4.1}\protected@file@percent }
\abx@aux@cite{autoattack}
\abx@aux@segm{0}{0}{autoattack}
\abx@aux@cite{lars}
\abx@aux@segm{0}{0}{lars}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\abx@aux@cite{autoattack}
\abx@aux@segm{0}{0}{autoattack}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results for Standard Fine-tuning the Readout Layer}{21}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Clean accuracies\relax }}{22}{figure.caption.14}\protected@file@percent }
\newlabel{fig:cleanaccuraciescifar10}{{13}{22}{Clean accuracies\relax }{figure.caption.14}{}}
\newlabel{fig:cleanaccuraciescifar10@cref}{{[figure][13][]13}{[1][21][]22}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The properties of different models used in the CIFAR10 experiments. Note variants of the same model (e.g simclr and supervised-simclr) have the same properties in this table. The simclr loss is also known as the Info-NCE or contrastive loss. The simclr and simsiam losses are for single samples and the final loss is the sum of all sample losses. For barlow twins and cross entropy loss the sum over samples is already included. For explanation of losses refer to \ref  {sec:ssl_methods} or their papers.\relax }}{22}{table.caption.15}\protected@file@percent }
\newlabel{tab:model_info}{{1}{22}{The properties of different models used in the CIFAR10 experiments. Note variants of the same model (e.g simclr and supervised-simclr) have the same properties in this table. The simclr loss is also known as the Info-NCE or contrastive loss. The simclr and simsiam losses are for single samples and the final loss is the sum of all sample losses. For barlow twins and cross entropy loss the sum over samples is already included. For explanation of losses refer to \ref {sec:ssl_methods} or their papers.\relax }{table.caption.15}{}}
\newlabel{tab:model_info@cref}{{[table][1][]1}{[1][21][]22}}
\abx@aux@cite{features_not_bugs_madry}
\abx@aux@segm{0}{0}{features_not_bugs_madry}
\newlabel{fig:sub1}{{14a}{23}{\relax }{figure.caption.16}{}}
\newlabel{fig:sub1@cref}{{[subfigure][1][14]14a}{[1][21][]23}}
\newlabel{sub@fig:sub1}{{a}{23}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:sub1@cref}{{[subfigure][1][14]14a}{[1][21][]23}}
\newlabel{fig:sub2}{{14b}{23}{\relax }{figure.caption.16}{}}
\newlabel{fig:sub2@cref}{{[subfigure][2][14]14b}{[1][21][]23}}
\newlabel{sub@fig:sub2}{{b}{23}{\relax }{figure.caption.16}{}}
\newlabel{sub@fig:sub2@cref}{{[subfigure][2][14]14b}{[1][21][]23}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Robust accuracies for readout standard tuning. (a) simCLR and barlow twins trained for 800 epochs, supervised and linear separated for 200 epochs. Linear separated is a supervised model with readout layer fine tuned like an SSL model. (b) simsiam and simclr-supervised trained for 400 epochs, supervised no-augmentation trained for 200 epochs.\relax }}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:standard_readout_results}{{14}{23}{Robust accuracies for readout standard tuning. (a) simCLR and barlow twins trained for 800 epochs, supervised and linear separated for 200 epochs. Linear separated is a supervised model with readout layer fine tuned like an SSL model. (b) simsiam and simclr-supervised trained for 400 epochs, supervised no-augmentation trained for 200 epochs.\relax }{figure.caption.16}{}}
\newlabel{fig:standard_readout_results@cref}{{[figure][14][]14}{[1][21][]23}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results for Adversarial Fine-tuning the Readout Layer}{23}{subsection.4.3}\protected@file@percent }
\newlabel{linear_adv_evals}{{4.3}{23}{Results for Adversarial Fine-tuning the Readout Layer}{subsection.4.3}{}}
\newlabel{linear_adv_evals@cref}{{[subsection][3][4]4.3}{[1][23][]23}}
\newlabel{fig:sub1}{{15a}{25}{\relax }{figure.caption.17}{}}
\newlabel{fig:sub1@cref}{{[subfigure][1][15]15a}{[1][24][]25}}
\newlabel{sub@fig:sub1}{{a}{25}{\relax }{figure.caption.17}{}}
\newlabel{sub@fig:sub1@cref}{{[subfigure][1][15]15a}{[1][24][]25}}
\newlabel{fig:sub2}{{15b}{25}{\relax }{figure.caption.17}{}}
\newlabel{fig:sub2@cref}{{[subfigure][2][15]15b}{[1][24][]25}}
\newlabel{sub@fig:sub2}{{b}{25}{\relax }{figure.caption.17}{}}
\newlabel{sub@fig:sub2@cref}{{[subfigure][2][15]15b}{[1][24][]25}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Caption for LOF}}{25}{figure.caption.17}\protected@file@percent }
\newlabel{fig:adv_readout_training}{{15}{25}{Caption for LOF}{figure.caption.17}{}}
\newlabel{fig:adv_readout_training@cref}{{[figure][15][]15}{[1][24][]25}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Feature difference norms for standard readout training.\relax }}{25}{table.caption.18}\protected@file@percent }
\newlabel{tb:std_readout_training}{{2}{25}{Feature difference norms for standard readout training.\relax }{table.caption.18}{}}
\newlabel{tb:std_readout_training@cref}{{[table][2][]2}{[1][24][]25}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Feature difference norms for adversarial readout training.\relax }}{25}{table.caption.19}\protected@file@percent }
\newlabel{tb:adv_readout_training}{{3}{25}{Feature difference norms for adversarial readout training.\relax }{table.caption.19}{}}
\newlabel{tb:adv_readout_training@cref}{{[table][3][]3}{[1][25][]25}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}CIFAR10 conclusion}{25}{subsection.4.4}\protected@file@percent }
\abx@aux@cite{ifm_ssl_avoid_shortcuts}
\abx@aux@segm{0}{0}{ifm_ssl_avoid_shortcuts}
\abx@aux@cite{ssl_loss_shortcuts}
\abx@aux@segm{0}{0}{ssl_loss_shortcuts}
\abx@aux@cite{contrastive_inverts_data_gen}
\abx@aux@segm{0}{0}{contrastive_inverts_data_gen}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Controlled datasets}{27}{section.5}\protected@file@percent }
\abx@aux@cite{concentration_measure}
\abx@aux@segm{0}{0}{concentration_measure}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Simple But Useful Toys}{28}{section.6}\protected@file@percent }
\newlabel{sec:toy_datasets}{{6}{28}{Simple But Useful Toys}{section.6}{}}
\newlabel{sec:toy_datasets@cref}{{[section][6][]6}{[1][28][]28}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}The triangle square dataset}{28}{subsection.6.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The hand made optimal projector.\relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{fig:trianglesquareoptimalprojector}{{16}{29}{The hand made optimal projector.\relax }{figure.caption.20}{}}
\newlabel{fig:trianglesquareoptimalprojector@cref}{{[figure][16][]16}{[1][29][]29}}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\newlabel{fig:toyclass1}{{17a}{30}{Triangle pattern\relax }{figure.caption.21}{}}
\newlabel{fig:toyclass1@cref}{{[subfigure][1][17]17a}{[1][30][]30}}
\newlabel{sub@fig:toyclass1}{{a}{30}{Triangle pattern\relax }{figure.caption.21}{}}
\newlabel{sub@fig:toyclass1@cref}{{[subfigure][1][17]17a}{[1][30][]30}}
\newlabel{fig:toyclass2}{{17b}{30}{Square pattern\relax }{figure.caption.21}{}}
\newlabel{fig:toyclass2@cref}{{[subfigure][2][17]17b}{[1][30][]30}}
\newlabel{sub@fig:toyclass2}{{b}{30}{Square pattern\relax }{figure.caption.21}{}}
\newlabel{sub@fig:toyclass2@cref}{{[subfigure][2][17]17b}{[1][30][]30}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The basic patterns\relax }}{30}{figure.caption.21}\protected@file@percent }
\newlabel{fig:Triangle_square_classes}{{17}{30}{The basic patterns\relax }{figure.caption.21}{}}
\newlabel{fig:Triangle_square_classes@cref}{{[figure][17][]17}{[1][30][]30}}
\newlabel{fig:sampletriangleimage}{{18a}{30}{An example of the triangle pattern on the canvas.\relax }{figure.caption.22}{}}
\newlabel{fig:sampletriangleimage@cref}{{[subfigure][1][18]18a}{[1][30][]30}}
\newlabel{sub@fig:sampletriangleimage}{{a}{30}{An example of the triangle pattern on the canvas.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:sampletriangleimage@cref}{{[subfigure][1][18]18a}{[1][30][]30}}
\newlabel{fig:toyambigiousimage}{{18b}{30}{An ambiguous image.\relax }{figure.caption.22}{}}
\newlabel{fig:toyambigiousimage@cref}{{[subfigure][2][18]18b}{[1][30][]30}}
\newlabel{sub@fig:toyambigiousimage}{{b}{30}{An ambiguous image.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:toyambigiousimage@cref}{{[subfigure][2][18]18b}{[1][30][]30}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Instances of images. The ambiguous image is not in the dataset.\relax }}{30}{figure.caption.22}\protected@file@percent }
\newlabel{fig:Triangle_square_image_samples}{{18}{30}{Instances of images. The ambiguous image is not in the dataset.\relax }{figure.caption.22}{}}
\newlabel{fig:Triangle_square_image_samples@cref}{{[figure][18][]18}{[1][30][]30}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The robustness of the model standardly trained on the triangle-square dataset for different values of $\epsilon $.\relax }}{31}{table.caption.23}\protected@file@percent }
\newlabel{tbl:std_model_trig_sqr}{{4}{31}{The robustness of the model standardly trained on the triangle-square dataset for different values of $\epsilon $.\relax }{table.caption.23}{}}
\newlabel{tbl:std_model_trig_sqr@cref}{{[table][4][]4}{[1][30][]31}}
\newlabel{fig:toyfilter1standard}{{19a}{31}{triangle\relax }{figure.caption.24}{}}
\newlabel{fig:toyfilter1standard@cref}{{[subfigure][1][19]19a}{[1][30][]31}}
\newlabel{sub@fig:toyfilter1standard}{{a}{31}{triangle\relax }{figure.caption.24}{}}
\newlabel{sub@fig:toyfilter1standard@cref}{{[subfigure][1][19]19a}{[1][30][]31}}
\newlabel{fig:toyfilter2standard}{{19b}{31}{square\relax }{figure.caption.24}{}}
\newlabel{fig:toyfilter2standard@cref}{{[subfigure][2][19]19b}{[1][30][]31}}
\newlabel{sub@fig:toyfilter2standard}{{b}{31}{square\relax }{figure.caption.24}{}}
\newlabel{sub@fig:toyfilter2standard@cref}{{[subfigure][2][19]19b}{[1][30][]31}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The standard training filters.\relax }}{31}{figure.caption.24}\protected@file@percent }
\newlabel{fig:toy_standard_training_filters}{{19}{31}{The standard training filters.\relax }{figure.caption.24}{}}
\newlabel{fig:toy_standard_training_filters@cref}{{[figure][19][]19}{[1][30][]31}}
\newlabel{fig:toyfilter1adv0}{{20a}{31}{triangle\relax }{figure.caption.25}{}}
\newlabel{fig:toyfilter1adv0@cref}{{[subfigure][1][20]20a}{[1][30][]31}}
\newlabel{sub@fig:toyfilter1adv0}{{a}{31}{triangle\relax }{figure.caption.25}{}}
\newlabel{sub@fig:toyfilter1adv0@cref}{{[subfigure][1][20]20a}{[1][30][]31}}
\newlabel{fig:toyfilter2adv0}{{20b}{31}{square\relax }{figure.caption.25}{}}
\newlabel{fig:toyfilter2adv0@cref}{{[subfigure][2][20]20b}{[1][30][]31}}
\newlabel{sub@fig:toyfilter2adv0}{{b}{31}{square\relax }{figure.caption.25}{}}
\newlabel{sub@fig:toyfilter2adv0@cref}{{[subfigure][2][20]20b}{[1][30][]31}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The adversarial training filters (FGSM). An interesting fact is that when I used perturbations bigger than 0.1, gradient descent was very unstable and could no longer optimize the loss effectively even when decreasing the learning rate by 4 orders of magnitude. This shows that adversarial training might have issues with big perturbation values since the loss landscape shifts significantly with each epoch which makes converging harder.\relax }}{31}{figure.caption.25}\protected@file@percent }
\newlabel{fig:toy_adv_training_filters}{{20}{31}{The adversarial training filters (FGSM). An interesting fact is that when I used perturbations bigger than 0.1, gradient descent was very unstable and could no longer optimize the loss effectively even when decreasing the learning rate by 4 orders of magnitude. This shows that adversarial training might have issues with big perturbation values since the loss landscape shifts significantly with each epoch which makes converging harder.\relax }{figure.caption.25}{}}
\newlabel{fig:toy_adv_training_filters@cref}{{[figure][20][]20}{[1][30][]31}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}The L-T dataset}{32}{subsection.6.2}\protected@file@percent }
\newlabel{ds_size_numeric_cals}{{6.2}{32}{The L-T dataset}{section*.26}{}}
\newlabel{ds_size_numeric_cals@cref}{{[subsection][2][6]6.2}{[1][32][]32}}
\newlabel{fig:L_pattern}{{21a}{33}{The L pattern\relax }{figure.caption.27}{}}
\newlabel{fig:L_pattern@cref}{{[subfigure][1][21]21a}{[1][33][]33}}
\newlabel{sub@fig:L_pattern}{{a}{33}{The L pattern\relax }{figure.caption.27}{}}
\newlabel{sub@fig:L_pattern@cref}{{[subfigure][1][21]21a}{[1][33][]33}}
\newlabel{fig:T_pattern}{{21b}{33}{The T pattern\relax }{figure.caption.27}{}}
\newlabel{fig:T_pattern@cref}{{[subfigure][2][21]21b}{[1][33][]33}}
\newlabel{sub@fig:T_pattern}{{b}{33}{The T pattern\relax }{figure.caption.27}{}}
\newlabel{sub@fig:T_pattern@cref}{{[subfigure][2][21]21b}{[1][33][]33}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A simpler dataset\relax }}{33}{figure.caption.27}\protected@file@percent }
\newlabel{fig:L-T-patterns}{{21}{33}{A simpler dataset\relax }{figure.caption.27}{}}
\newlabel{fig:L-T-patterns@cref}{{[figure][21][]21}{[1][33][]33}}
\newlabel{fig:lfilterstdtrainingstddataset20epochs}{{22a}{33}{trained for 20 epochs\relax }{figure.caption.28}{}}
\newlabel{fig:lfilterstdtrainingstddataset20epochs@cref}{{[subfigure][1][22]22a}{[1][33][]33}}
\newlabel{sub@fig:lfilterstdtrainingstddataset20epochs}{{a}{33}{trained for 20 epochs\relax }{figure.caption.28}{}}
\newlabel{sub@fig:lfilterstdtrainingstddataset20epochs@cref}{{[subfigure][1][22]22a}{[1][33][]33}}
\newlabel{fig:lfilterstdtrainingstddata200epochs}{{22b}{33}{trained for 200 epochs\relax }{figure.caption.28}{}}
\newlabel{fig:lfilterstdtrainingstddata200epochs@cref}{{[subfigure][2][22]22b}{[1][33][]33}}
\newlabel{sub@fig:lfilterstdtrainingstddata200epochs}{{b}{33}{trained for 200 epochs\relax }{figure.caption.28}{}}
\newlabel{sub@fig:lfilterstdtrainingstddata200epochs@cref}{{[subfigure][2][22]22b}{[1][33][]33}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces L filter results for standard model training using standard data (8 images)\relax }}{33}{figure.caption.28}\protected@file@percent }
\newlabel{fig:standard_L_filters}{{22}{33}{L filter results for standard model training using standard data (8 images)\relax }{figure.caption.28}{}}
\newlabel{fig:standard_L_filters@cref}{{[figure][22][]22}{[1][33][]33}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Simplifying assumptions}{33}{subsubsection.6.2.1}\protected@file@percent }
\newlabel{assumption3}{{6.2.1}{34}{Simplifying assumptions}{section*.29}{}}
\newlabel{assumption3@cref}{{[subsubsection][1][6,2]6.2.1}{[1][34][]34}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Proof of model robustness}{34}{subsubsection.6.2.2}\protected@file@percent }
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\newlabel{fig:robustmodelLfilter}{{23a}{35}{L filter\relax }{figure.caption.30}{}}
\newlabel{fig:robustmodelLfilter@cref}{{[subfigure][1][23]23a}{[1][35][]35}}
\newlabel{sub@fig:robustmodelLfilter}{{a}{35}{L filter\relax }{figure.caption.30}{}}
\newlabel{sub@fig:robustmodelLfilter@cref}{{[subfigure][1][23]23a}{[1][35][]35}}
\newlabel{fig:robustmodelTfilter}{{23b}{35}{T filter\relax }{figure.caption.30}{}}
\newlabel{fig:robustmodelTfilter@cref}{{[subfigure][2][23]23b}{[1][35][]35}}
\newlabel{sub@fig:robustmodelTfilter}{{b}{35}{T filter\relax }{figure.caption.30}{}}
\newlabel{sub@fig:robustmodelTfilter@cref}{{[subfigure][2][23]23b}{[1][35][]35}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The filters learned by the robust model.\relax }}{35}{figure.caption.30}\protected@file@percent }
\newlabel{fig:robust_model_filters}{{23}{35}{The filters learned by the robust model.\relax }{figure.caption.30}{}}
\newlabel{fig:robust_model_filters@cref}{{[figure][23][]23}{[1][35][]35}}
\newlabel{fig:standardtimage}{{24a}{36}{a standard T image\relax }{figure.caption.31}{}}
\newlabel{fig:standardtimage@cref}{{[subfigure][1][24]24a}{[1][35][]36}}
\newlabel{sub@fig:standardtimage}{{a}{36}{a standard T image\relax }{figure.caption.31}{}}
\newlabel{sub@fig:standardtimage@cref}{{[subfigure][1][24]24a}{[1][35][]36}}
\newlabel{fig:advresarialtimage}{{24b}{36}{corresponding adversarial image found by PGD\relax }{figure.caption.31}{}}
\newlabel{fig:advresarialtimage@cref}{{[subfigure][2][24]24b}{[1][35][]36}}
\newlabel{sub@fig:advresarialtimage}{{b}{36}{corresponding adversarial image found by PGD\relax }{figure.caption.31}{}}
\newlabel{sub@fig:advresarialtimage@cref}{{[subfigure][2][24]24b}{[1][35][]36}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces A counter example as calculated by PGD\relax }}{36}{figure.caption.31}\protected@file@percent }
\newlabel{fig:pgd_counterexample}{{24}{36}{A counter example as calculated by PGD\relax }{figure.caption.31}{}}
\newlabel{fig:pgd_counterexample@cref}{{[figure][24][]24}{[1][35][]36}}
\newlabel{fig:usingassumption4lfilter}{{25a}{36}{The L filter\relax }{figure.caption.32}{}}
\newlabel{fig:usingassumption4lfilter@cref}{{[subfigure][1][25]25a}{[1][35][]36}}
\newlabel{sub@fig:usingassumption4lfilter}{{a}{36}{The L filter\relax }{figure.caption.32}{}}
\newlabel{sub@fig:usingassumption4lfilter@cref}{{[subfigure][1][25]25a}{[1][35][]36}}
\newlabel{fig:usingassumption4tfilter}{{25b}{36}{The T filter\relax }{figure.caption.32}{}}
\newlabel{fig:usingassumption4tfilter@cref}{{[subfigure][2][25]25b}{[1][35][]36}}
\newlabel{sub@fig:usingassumption4tfilter}{{b}{36}{The T filter\relax }{figure.caption.32}{}}
\newlabel{sub@fig:usingassumption4tfilter@cref}{{[subfigure][2][25]25b}{[1][35][]36}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The filters learned by the model using assumption 4.\relax }}{36}{figure.caption.32}\protected@file@percent }
\newlabel{fig:assumption4_model}{{25}{36}{The filters learned by the model using assumption 4.\relax }{figure.caption.32}{}}
\newlabel{fig:assumption4_model@cref}{{[figure][25][]25}{[1][35][]36}}
\abx@aux@cite{adv_training_madry}
\abx@aux@segm{0}{0}{adv_training_madry}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The robust model maintains 100\% robustness even when $Dataset\text  {-}\epsilon <Attack\text  {-}\epsilon $. The $Dataset\text  {-}\epsilon =4.1$ is fixed. The dots show the values calculated by PGD the curve is regressed using a polynomial of degree 5. We see that the model maintains robustness even after $Dataset\text  {-}\epsilon < Attack\text  {-}\epsilon $. It should be the case that for $Attack\text  {-}\epsilon = 0.5$ we get 0 accuracy since the samples become ambiguous. I also tried using PGD with $\alpha = 0.4*0.1$ fixed and the results were the same.\relax }}{37}{figure.caption.33}\protected@file@percent }
\newlabel{fig:accuracybigepsilonrangerobustness}{{26}{37}{The robust model maintains 100\% robustness even when $\dseps <\attackeps $. The $\dseps =4.1$ is fixed. The dots show the values calculated by PGD the curve is regressed using a polynomial of degree 5. We see that the model maintains robustness even after $\dseps < \attackeps $. It should be the case that for $\attackeps = 0.5$ we get 0 accuracy since the samples become ambiguous. I also tried using PGD with $\alpha = 0.4*0.1$ fixed and the results were the same.\relax }{figure.caption.33}{}}
\newlabel{fig:accuracybigepsilonrangerobustness@cref}{{[figure][26][]26}{[1][37][]37}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Observations from the Triangle-Square and L-T datasets}{37}{subsubsection.6.2.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Evaluating robustness with more $Attack\text  {-}\epsilon $ precision (zooming in on the \ref  {fig:accuracybigepsilonrangerobustness} plot) after the robust model starts losing accuracy ($0.46\leq Attack\text  {-}\epsilon $). The $Dataset\text  {-}\epsilon =4.1$ is fixed. The dots show the values calculated by PGD the curve is regressed using a polynomial of degree 8. We can see the drop in accuracy is slow.\relax }}{38}{figure.caption.34}\protected@file@percent }
\newlabel{fig:accuracysmallepsilonrangerobustness}{{27}{38}{Evaluating robustness with more $\attackeps $ precision (zooming in on the \ref {fig:accuracybigepsilonrangerobustness} plot) after the robust model starts losing accuracy ($0.46\leq \attackeps $). The $\dseps =4.1$ is fixed. The dots show the values calculated by PGD the curve is regressed using a polynomial of degree 8. We can see the drop in accuracy is slow.\relax }{figure.caption.34}{}}
\newlabel{fig:accuracysmallepsilonrangerobustness@cref}{{[figure][27][]27}{[1][37][]38}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces short}}{39}{figure.caption.35}\protected@file@percent }
\newlabel{fig:weneedallsamplesinboundaryintuition}{{28}{39}{short}{figure.caption.35}{}}
\newlabel{fig:weneedallsamplesinboundaryintuition@cref}{{[figure][28][]28}{[1][39][]39}}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Knowing the causes is not enough ! (Causal3DIdent experiments)}{40}{section.7}\protected@file@percent }
\newlabel{sec:3dident}{{7}{40}{Knowing the causes is not enough ! (Causal3DIdent experiments)}{section.7}{}}
\newlabel{sec:3dident@cref}{{[section][7][]7}{[1][40][]40}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Dataset creation}{40}{subsection.7.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces (Left) Causal graph for the Causal3DIdent dataset. (Right) Two samples from each object class.\relax }}{41}{figure.caption.36}\protected@file@percent }
\newlabel{fig:3dident_explain}{{29}{41}{(Left) Causal graph for the Causal3DIdent dataset. (Right) Two samples from each object class.\relax }{figure.caption.36}{}}
\newlabel{fig:3dident_explain@cref}{{[figure][29][]29}{[1][41][]41}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}General idea}{41}{subsection.7.2}\protected@file@percent }
\newlabel{sec:3dident_general_idea}{{7.2}{41}{General idea}{subsection.7.2}{}}
\newlabel{sec:3dident_general_idea@cref}{{[subsection][2][7]7.2}{[1][41][]41}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces The spotlight rotation is divided into 3 non-overlapping classes. Each class has the same range and the gaps between classes are equal in length. Spotlight rotation is sampled uniformly from the whole range $[0,\pi ]$ so the classes are balanced. Since the classes don't overlap there exists a model that is robust. The most robust classifier would separate the classes using the middle point of gaps (red dashed lines). Therefore $\delta ^*$ is the maximum perturbation that can be tolerated by this classification task.\relax }}{42}{figure.caption.37}\protected@file@percent }
\newlabel{fig:spotlight_rotation_classes}{{30}{42}{The spotlight rotation is divided into 3 non-overlapping classes. Each class has the same range and the gaps between classes are equal in length. Spotlight rotation is sampled uniformly from the whole range $[0,\pi ]$ so the classes are balanced. Since the classes don't overlap there exists a model that is robust. The most robust classifier would separate the classes using the middle point of gaps (red dashed lines). Therefore $\delta ^*$ is the maximum perturbation that can be tolerated by this classification task.\relax }{figure.caption.37}{}}
\newlabel{fig:spotlight_rotation_classes@cref}{{[figure][30][]30}{[1][41][]42}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces The classifier including the regressor and the classification head. Note that the classification head is purely defined by our definition of classes and is not learned.\relax }}{42}{figure.caption.38}\protected@file@percent }
\newlabel{fig:regressor_classifier}{{31}{42}{The classifier including the regressor and the classification head. Note that the classification head is purely defined by our definition of classes and is not learned.\relax }{figure.caption.38}{}}
\newlabel{fig:regressor_classifier@cref}{{[figure][31][]31}{[1][41][]42}}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\abx@aux@cite{3dident}
\abx@aux@segm{0}{0}{3dident}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiments}{43}{subsection.7.3}\protected@file@percent }
\newlabel{sec:spotlight_exp}{{7.3}{43}{Experiments}{subsection.7.3}{}}
\newlabel{sec:spotlight_exp@cref}{{[subsection][3][7]7.3}{[1][43][]43}}
\abx@aux@cite{underspecification}
\abx@aux@segm{0}{0}{underspecification}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The $R^2$ scores for different image features for Simclr. First row results for a linear readout, second row corresponds to using kernel ridge regression. Note since I used color distortion in training simclr the scores for the 3 color related attributes (object, spotlight, background) were negative or close to 0 and excluded from the table. We can see that linear readout gives the best results.\relax }}{44}{table.caption.39}\protected@file@percent }
\newlabel{tbl:R2_results}{{5}{44}{The $R^2$ scores for different image features for Simclr. First row results for a linear readout, second row corresponds to using kernel ridge regression. Note since I used color distortion in training simclr the scores for the 3 color related attributes (object, spotlight, background) were negative or close to 0 and excluded from the table. We can see that linear readout gives the best results.\relax }{table.caption.39}{}}
\newlabel{tbl:R2_results@cref}{{[table][5][]5}{[1][43][]44}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{46}{section.8}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{3B8C58D96C5666F8571DD11BDBE588D3}
\abx@aux@read@bblrerun
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{excessive_invariance}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{adv_exp_original_paper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{3dident}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{grad_align}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{cure}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{features_not_bugs_madry}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{adv_training_plus_ssl_training}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{contrast_to_divide}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{label_noise_contrastive_learning}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{goodfellow_fgsm}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{autoattack}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{adv_training_madry}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{on_manifold_off_manifold}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rotation_gidaris_unsupervised_2018}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{arora_theoretical_2019}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{simclr_chen_simple_2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{barlow_twins}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{contrastive_learning_spectral_proof}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{supervised_texture_bias}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{contrastive_loss_shortcut_solutions}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ELR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ifm_ssl_avoid_shortcuts}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ACL_Adversarial_Contrastive_Learning_2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{shortcut_in_contrastive_losses}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{robustness_at_odds_madry}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bolts}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{huy_phan_2021_4431043}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{simsiam}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{lars}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ssl_loss_shortcuts}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{contrastive_inverts_data_gen}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{concentration_measure}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{underspecification}{none/global//global/global}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{1}}
\gdef \@abspage@last{49}
