# Adversarial examples and unsupervised learning

* The loss function is supposed to be a proxy for the accuracy measure used to analyse a model. But this proxy is not always perfect.

* I explore how the definition of the loss function and the specific method of learning used (self-supervised or supervised) affects the robustness of a model to adversarial examples.

* A toy dataset such as 3dident could help us isolate the factors that contribute to adversarial susceptibility.  
